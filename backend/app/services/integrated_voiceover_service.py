"""
VoxChina å¤šæ–‡çŒ®æ•´åˆå£æ’­ç¨¿ä»¶ç­–åˆ’æœåŠ¡ (Evidence-based Scriptwriter)
åŸºäºç”¨æˆ·ä¸Šä¼ çš„å¤šç¯‡æ–‡æ¡£ï¼Œç”Ÿæˆå¯ç›´æ¥å½•åˆ¶çš„çŸ­è§†é¢‘å£æ’­ç¨¿
"""
import uuid
import json
import re
import asyncio
import os
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
from loguru import logger
from app.services.llm_service import llm_service
from app.services.document_parser_service import document_parser_service
from app.models.schemas import (
    IntegratedVoiceoverRequest,
    IntegratedVoiceoverResponse,
    EvidenceLedger,
    EvidenceFinding,
    VisualAssetLedger,
    VisualAsset,
    StyleProfile
)


class IntegratedVoiceoverService:
    """æ•´åˆå£æ’­ç¨¿ä»¶ç”ŸæˆæœåŠ¡"""
    
    def __init__(self):
        # æŒä¹…åŒ–æ–‡ä»¶è·¯å¾„
        self.tasks_file = Path("/www/wwwroot/voxchina/backend/static/integrated_voiceover_tasks.json")
        
        # ä»»åŠ¡å­˜å‚¨ {task_id: task_data}
        self.tasks = {}
        
        # åŠ è½½å†å²ä»»åŠ¡
        self._load_tasks()
    
    def _save_tasks(self):
        """ä¿å­˜ä»»åŠ¡æ•°æ®åˆ°æ–‡ä»¶"""
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            self.tasks_file.parent.mkdir(parents=True, exist_ok=True)
            
            # åºåˆ—åŒ–ä»»åŠ¡æ•°æ®
            tasks_data = {}
            for task_id, task in self.tasks.items():
                # æ·±æ‹·è´ä»»åŠ¡æ•°æ®
                task_copy = {}
                for key, value in task.items():
                    # è½¬æ¢æ—¥æœŸæ—¶é—´ä¸ºå­—ç¬¦ä¸²
                    if isinstance(value, datetime):
                        task_copy[key] = value.isoformat()
                    else:
                        task_copy[key] = value
                tasks_data[task_id] = task_copy
            
            # å†™å…¥æ–‡ä»¶ï¼ˆä¸´æ—¶æ–‡ä»¶ + åŸå­æ›¿æ¢ï¼‰
            temp_file = self.tasks_file.with_suffix('.tmp')
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(tasks_data, f, ensure_ascii=False, indent=2, default=str)
            
            # åŸå­æ›¿æ¢
            temp_file.replace(self.tasks_file)
            
            logger.debug(f"âœ… å·²ä¿å­˜ {len(self.tasks)} ä¸ªä»»åŠ¡")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
    
    def _load_tasks(self):
        """ä»æ–‡ä»¶åŠ è½½ä»»åŠ¡æ•°æ®"""
        try:
            if not self.tasks_file.exists():
                logger.info("ğŸ“‚ ä»»åŠ¡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä»ç©ºå¼€å§‹")
                return
            
            with open(self.tasks_file, 'r', encoding='utf-8') as f:
                tasks_data = json.load(f)
            
            # æ¢å¤ä»»åŠ¡æ•°æ®
            for task_id, task in tasks_data.items():
                # è½¬æ¢å­—ç¬¦ä¸²ä¸ºæ—¥æœŸæ—¶é—´
                if 'created_at' in task and isinstance(task['created_at'], str):
                    task['created_at'] = datetime.fromisoformat(task['created_at'])
                if 'updated_at' in task and isinstance(task['updated_at'], str):
                    task['updated_at'] = datetime.fromisoformat(task['updated_at'])
                self.tasks[task_id] = task
            
            logger.info(f"âœ… å·²åŠ è½½ {len(self.tasks)} ä¸ªå†å²ä»»åŠ¡")
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½ä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
            self.tasks = {}
    
    async def _call_llm_with_retry(
        self,
        prompt: str,
        timeout: float = 300.0,
        max_retries: int = 3,
        step_name: str = "LLMè°ƒç”¨"
    ) -> str:
        """
        å¸¦è‡ªåŠ¨é‡è¯•çš„LLMè°ƒç”¨
        
        Args:
            prompt: æç¤ºè¯
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            step_name: æ­¥éª¤åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        
        Returns:
            LLMå“åº”æ–‡æœ¬
        """
        last_error = None
        
        for attempt in range(max_retries):
            try:
                logger.info(f"{step_name} - å°è¯• {attempt + 1}/{max_retries}")
                
                response = await llm_service._generate_with_provider(
                    prompt=prompt,
                    timeout=timeout
                )
                
                logger.info(f"{step_name} - æˆåŠŸ")
                return response
                
            except Exception as e:
                last_error = e
                error_msg = str(e)
                logger.warning(f"{step_name} - å°è¯• {attempt + 1} å¤±è´¥: {error_msg}")
                
                # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…åé‡è¯•
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 5  # 5ç§’, 10ç§’, 15ç§’...
                    logger.info(f"{step_name} - ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    await asyncio.sleep(wait_time)
                else:
                    logger.error(f"{step_name} - æ‰€æœ‰é‡è¯•å‡å¤±è´¥")
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        raise last_error
        
    async def create_task(
        self,
        request: IntegratedVoiceoverRequest,
        files: List[tuple]  # [(filename, file_content), ...]
    ) -> str:
        """
        åˆ›å»ºæ–°çš„å£æ’­ç¨¿ç”Ÿæˆä»»åŠ¡
        
        Args:
            request: è¯·æ±‚å‚æ•°
            files: ä¸Šä¼ çš„æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            task_id: ä»»åŠ¡ID
        """
        task_id = str(uuid.uuid4())
        
        # åˆå§‹åŒ–ä»»åŠ¡
        task_data = {
            "task_id": task_id,
            "status": "processing",
            "progress": 0,
            "current_step": "Step0",
            "request": request.dict(),
            "files": [],
            "parsed_docs": [],
            "style_profile": None,
            "evidence_ledger": [],
            "visual_asset_ledger": None,
            "structure": None,
            "script_review": None,
            "script_final": None,
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat()
        }
        
        # ä¿å­˜æ–‡ä»¶ä¿¡æ¯
        for filename, file_content in files:
            task_data["files"].append({
                "filename": filename,
                "size": len(file_content)
            })
        
        self.tasks[task_id] = task_data
        
        # ä¿å­˜ä»»åŠ¡åˆ°æ–‡ä»¶
        self._save_tasks()
        
        # å¯åŠ¨åå°å¼‚æ­¥å¤„ç†ä»»åŠ¡ï¼ˆä¸ç­‰å¾…å®Œæˆï¼‰
        import asyncio
        asyncio.create_task(self._process_task_wrapper(task_id, request, files))
        
        return task_id
    
    async def _process_task_wrapper(
        self,
        task_id: str,
        request: IntegratedVoiceoverRequest,
        files: List[tuple]
    ):
        """åŒ…è£…å™¨ï¼šå¤„ç†ä»»åŠ¡å¹¶æ•è·å¼‚å¸¸"""
        try:
            await self._process_task(task_id, request, files)
        except Exception as e:
            logger.error(f"Task {task_id} failed: {e}")
            task_data = self.tasks.get(task_id)
            if task_data:
                task_data["status"] = "failed"
                task_data["error"] = str(e)
                task_data["updated_at"] = datetime.now().isoformat()
                self._save_tasks()  # ä¿å­˜ä»»åŠ¡çŠ¶æ€
    
    async def _process_task(
        self,
        task_id: str,
        request: IntegratedVoiceoverRequest,
        files: List[tuple]
    ):
        """å¤„ç†ä»»åŠ¡çš„ä¸»æµç¨‹"""
        task_data = self.tasks[task_id]
        
        try:
            # Step 0: è§£ææ–‡æ¡£
            logger.info(f"Task {task_id}: Parsing documents...")
            task_data["current_step"] = "Parsing"
            task_data["progress"] = 5
            task_data["updated_at"] = datetime.now().isoformat()
            
            parsed_docs = []
            for filename, file_content in files:
                try:
                    parsed_doc = document_parser_service.parse_document(file_content, filename)
                    doc_id = f"D{len(parsed_docs) + 1}"
                    parsed_doc["doc_id"] = doc_id
                    
                    # è¯¦ç»†è®°å½•å›¾ç‰‡ä¿¡æ¯
                    images_count = len(parsed_doc.get("images", []))
                    logger.info(f"ğŸ“„ æ–‡æ¡£ {doc_id} ({filename}): {images_count} å¼ å›¾ç‰‡")
                    if images_count > 0:
                        for idx, img in enumerate(parsed_doc.get("images", [])):
                            logger.info(f"   - å›¾ç‰‡ {idx+1}: {img.get('url')}")
                    
                    parsed_docs.append(parsed_doc)
                except Exception as e:
                    logger.error(f"Failed to parse {filename}: {e}")
                    continue
            
            logger.info(f"ğŸ“¦ æ€»å…±è§£æäº† {len(parsed_docs)} ä¸ªæ–‡æ¡£")
            task_data["parsed_docs"] = parsed_docs
            task_data["progress"] = 10
            task_data["updated_at"] = datetime.now().isoformat()
            
            # Step 0: Style Profile
            logger.info(f"Task {task_id}: Generating Style Profile...")
            task_data["current_step"] = "Step0"
            style_profile = await self._generate_style_profile(request, parsed_docs)
            task_data["style_profile"] = style_profile
            task_data["progress"] = 20
            task_data["updated_at"] = datetime.now().isoformat()
            
            # Step A: Evidence Ledger
            logger.info(f"Task {task_id}: Building Evidence Ledger...")
            task_data["current_step"] = "StepA"
            evidence_ledger = await self._build_evidence_ledger(parsed_docs, request.topic_hint)
            task_data["evidence_ledger"] = evidence_ledger
            task_data["progress"] = 35
            task_data["updated_at"] = datetime.now().isoformat()
            
            # Step A2: Visual Asset Ledger
            logger.info(f"Task {task_id}: Building Visual Asset Ledger...")
            task_data["current_step"] = "StepA2"
            visual_asset_ledger = await self._build_visual_asset_ledger(parsed_docs, evidence_ledger)
            
            # è¯¦ç»†è®°å½• Visual Asset Ledger å†…å®¹
            logger.info(f"ğŸ“Š Visual Asset Ledger æ„å»ºå®Œæˆ:")
            logger.info(f"   - æ€»èµ„äº§æ•°: {len(visual_asset_ledger.get('assets', []))}")
            for asset in visual_asset_ledger.get('assets', []):
                logger.info(f"   - {asset['asset_id']} ({asset['asset_type']}): image_url={'å­˜åœ¨' if 'image_url' in asset else 'ä¸å­˜åœ¨'}")
                if 'image_url' in asset:
                    logger.info(f"      URL: {asset['image_url']}")
            
            task_data["visual_asset_ledger"] = visual_asset_ledger
            task_data["progress"] = 50
            task_data["updated_at"] = datetime.now().isoformat()
            
            # Step B: Structure Selector
            logger.info(f"Task {task_id}: Selecting Structure...")
            task_data["current_step"] = "StepB"
            structure = await self._select_structure(
                request.topic_hint,
                evidence_ledger,
                visual_asset_ledger,
                style_profile,
                request.style_preference
            )
            task_data["structure"] = structure
            task_data["progress"] = 65
            task_data["updated_at"] = datetime.now().isoformat()
            
            # Step C: Script Review Version
            logger.info(f"Task {task_id}: Generating Script (Review Version)...")
            task_data["current_step"] = "StepC"
            script_review = await self._generate_script_review(
                request,
                style_profile,
                evidence_ledger,
                visual_asset_ledger,
                structure
            )
            task_data["script_review"] = script_review
            task_data["progress"] = 85
            task_data["updated_at"] = datetime.now().isoformat()
            
            # Step D: Script Final Version
            logger.info(f"Task {task_id}: Generating Script (Final Version)...")
            task_data["current_step"] = "StepD"
            script_final = await self._generate_script_final(script_review)
            task_data["script_final"] = script_final
            task_data["progress"] = 100
            task_data["status"] = "completed"
            task_data["updated_at"] = datetime.now().isoformat()
            
            # ä¿å­˜ä»»åŠ¡çŠ¶æ€
            self._save_tasks()
            
            logger.info(f"Task {task_id}: Completed successfully")
            
        except Exception as e:
            logger.error(f"Task {task_id} processing failed at {task_data.get('current_step', 'Unknown')}: {e}")
            task_data["status"] = "failed"
            
            # æä¾›æ›´å‹å¥½çš„é”™è¯¯ä¿¡æ¯
            current_step = task_data.get('current_step', 'Unknown')
            step_names = {
                'Parsing': 'æ–‡æ¡£è§£æ',
                'Step0': 'é£æ ¼é…ç½®ç”Ÿæˆ',
                'StepA': 'è¯æ®æå–',
                'StepA2': 'å›¾è¡¨è¯†åˆ«',
                'StepB': 'ç»“æ„è®¾è®¡',
                'StepC': 'å®¡é˜…ç‰ˆç”Ÿæˆ',
                'StepD': 'ä¸Šå±ç‰ˆç”Ÿæˆ'
            }
            
            step_name = step_names.get(current_step, current_step)
            error_msg = f"å¤„ç†å¤±è´¥äºã€{step_name}ã€‘æ­¥éª¤"
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯è¶…æ—¶é”™è¯¯
            if "timeout" in str(e).lower() or "timed out" in str(e).lower():
                error_msg += " - LLMå“åº”è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•"
            elif "rate limit" in str(e).lower():
                error_msg += " - APIè°ƒç”¨é¢‘ç‡é™åˆ¶ï¼Œè¯·ç¨åé‡è¯•"
            else:
                error_msg += f" - {str(e)}"
            
            task_data["error"] = error_msg
            task_data["updated_at"] = datetime.now().isoformat()
            self._save_tasks()  # ä¿å­˜ä»»åŠ¡çŠ¶æ€
            raise
    
    async def _generate_style_profile(
        self,
        request: IntegratedVoiceoverRequest,
        parsed_docs: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Step 0: ç”Ÿæˆé£æ ¼é…ç½®"""
        
        # æ„å»ºæ–‡æ¡£æ¦‚è§ˆ
        docs_overview = "\n\n".join([
            f"æ–‡æ¡£{doc['doc_id']}: {doc['title']}\næ®µè½æ•°: {doc['total_paragraphs']}"
            for doc in parsed_docs
        ])
        
        prompt = f"""
ä½ æ˜¯VoxChinaå£æ’­ç¨¿é£æ ¼åˆ†æä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ï¼Œç¡®å®šæœ¬æ¬¡å£æ’­ç¨¿çš„é£æ ¼é…ç½®ã€‚

ä¸»é¢˜æç¤º: {request.topic_hint}
æ–‡æ¡£æ¦‚è§ˆ:
{docs_overview}

è¯·è¾“å‡ºä»¥ä¸‹å†…å®¹ï¼ˆä¸¥æ ¼JSONæ ¼å¼ï¼Œä¸è¦é™„åŠ ä»»ä½•è§£é‡Šï¼‰:
{{
    "enable_vox_intro": {str(request.include_vox_intro).lower()},
    "main_structure": "S1/S2/S3/S4ä¹‹ä¸€",
    "figure_style": "Aæˆ–B",
    "rules": ["è§„åˆ™1", "è§„åˆ™2", "è§„åˆ™3", "è§„åˆ™4", "è§„åˆ™5"]
}}

ã€ç»“æ„è¯´æ˜ã€‘
- S1: ä¸‰ç»´åº¦/ä¸‰ä¸»çº¿ï¼ˆé€‚åˆå¤šè§’åº¦åˆ†æï¼‰
- S2: æ—¶é—´çº¿/é˜¶æ®µæ¨è¿›ï¼ˆé€‚åˆå†å²æ¼”è¿›ï¼‰
- S3: ç°çŠ¶â€”æœºåˆ¶â€”å¯¹ç­–ï¼ˆé€‚åˆæ”¿ç­–åˆ†æï¼‰
- S4: æœºåˆ¶é“¾æ¡ï¼ˆé€‚åˆå› æœåˆ†æï¼‰

ã€å›¾è¡¨é£æ ¼ã€‘
- A: ç‹¬ç«‹è¡Œå±•ç¤ºï¼ˆå›¾/è¡¨ + æ ‡é¢˜ + è¦ç‚¹ï¼‰
- B: æ­£æ–‡å†…åµŒï¼ˆã€”ç”»é¢ï¼šasset_idï¼ˆæ ‡é¢˜ï¼‰â€” è¦ç‚¹ã€•ï¼‰

ã€STYLE RULES ç”Ÿæˆè¦æ±‚â€”â€”å¿…é¡»ä¸¥æ ¼éµå®ˆã€‘
rules æ•°ç»„å¿…é¡»æ°å¥½åŒ…å« 5 æ¡è§„åˆ™ï¼Œæ¯æ¡è§„åˆ™å¿…é¡»æ˜¯**é’ˆå¯¹æœ¬æ¬¡æ–‡æ¡£ä¸»é¢˜çš„å…·ä½“æŒ‡å¯¼**ï¼Œä¸å…è®¸å†™æ³›æ³›çš„é€šç”¨è§„åˆ™ã€‚

5 æ¡è§„åˆ™å¿…é¡»åˆ†åˆ«è¦†ç›–ä»¥ä¸‹ 5 ä¸ªç»´åº¦ï¼ˆæ¯ä¸ªç»´åº¦ 1 æ¡ï¼‰:
1. å™äº‹é€»è¾‘ï¼šæè¿°æ•´ä½“å£æ’­çš„å™äº‹æ¨è¿›æ–¹å¼ï¼Œè¦ç»“åˆå…·ä½“ä¸»é¢˜è¯´æ˜é‡‡ç”¨ä»€ä¹ˆè®ºè¯é€»è¾‘ï¼ˆå¦‚å› æœé“¾æ¡ã€å¯¹æ¯”åˆ†æã€æ”¿ç­–æ¨æ¼”ç­‰ï¼‰ï¼Œæ˜ç¡®å„æ®µçš„é€»è¾‘è¡”æ¥æ–¹å¼
2. æœºåˆ¶é“¾/è®ºè¯é“¾ï¼šè¯´æ˜æ ¸å¿ƒè®ºè¯é“¾çš„æ„æˆè¦ç´ â€”â€”èµ·ç‚¹å˜é‡ã€ä¸­ä»‹æœºåˆ¶ã€ç»“æœå˜é‡åˆ†åˆ«æ˜¯ä»€ä¹ˆï¼Œè¦ç”¨æ–‡æ¡£ä¸­çš„å®é™…æ¦‚å¿µå¡«å……
3. å¼•ç”¨è§„èŒƒï¼šè¦æ±‚å…³é”®ç»“è®ºå¿…é¡»å¼•ç”¨å…·ä½“ç ”ç©¶æˆ–æ”¿ç­–æ¥æºï¼Œè¯´æ˜å¼•ç”¨çš„ä¼˜å…ˆçº§ï¼ˆå¦‚ä¼˜å…ˆå¼•ç”¨å“ªäº›æ–‡æ¡£çš„å®è¯ç»“æœï¼Œæ”¿ç­–èƒŒæ™¯å’Œç°å®ä¸¾ä¾‹å¯è¾…ä»¥å“ªäº›æ–‡æ¡£ï¼‰
4. æ•°æ®å£å¾„ï¼šæ¶‰åŠæ•°å­—æ—¶å¿…é¡»è¯´æ˜æ—¶é—´ã€åœ°åŒºã€æ ·æœ¬æˆ–æ”¿ç­–èƒŒæ™¯ç­‰å£å¾„ä¿¡æ¯ï¼›æ²¡æœ‰ç¡®åˆ‡æ•°å­—æ—¶é¿å…"ç¿»å€""å¤§å¹…"ç­‰æ¨¡ç³Šè¡¨è¿°
5. è¯æ®ä¼˜å…ˆçº§ï¼šè¯´æ˜å„æ–‡æ¡£ï¼ˆD1-D{len(parsed_docs)}ï¼‰çš„ä½¿ç”¨ç­–ç•¥â€”â€”å“ªäº›æ–‡æ¡£çš„å®è¯ç»“æœä½œä¸ºæ ¸å¿ƒè¯æ®ï¼Œå“ªäº›ä½œä¸ºèƒŒæ™¯è¡¥å……

ã€å¥½è§„åˆ™ vs å·®è§„åˆ™ç¤ºä¾‹ã€‘
å·®è§„åˆ™ï¼ˆå¤ªæ³›ï¼Œç¦æ­¢ï¼‰: "æ¯æ®µå¿…é¡»æœ‰è¯æ®æ”¯æ’‘"
å¥½è§„åˆ™ï¼ˆå…·ä½“åˆ°ä¸»é¢˜ï¼‰: "æ•´ä½“é‡‡ç”¨'é—®é¢˜æå‡ºâ€”æœºåˆ¶æ‹†è§£â€”å®è¯è¯æ®â€”å½±å“ä¸å¯ç¤º'çš„å£æ’­é€»è¾‘ï¼Œçªå‡ºå› æœé“¾æ¡"

å·®è§„åˆ™ï¼ˆå¤ªæ³›ï¼Œç¦æ­¢ï¼‰: "ç¦æ­¢å¤–æ¨å’Œä¸»è§‚æ¨æ–­"
å¥½è§„åˆ™ï¼ˆå…·ä½“åˆ°ä¸»é¢˜ï¼‰: "æ¯ä¸€æ¡æœºåˆ¶é“¾éƒ½è¦æ˜ç¡®ï¼šèµ·ç‚¹å˜é‡ï¼ˆå¦‚æ•°å­—åˆ†å¿ƒ/å…è´¹æ•™è‚²ï¼‰â€”ä¸­ä»‹æœºåˆ¶ï¼ˆè¡Œä¸ºæ”¹å˜ã€èµ„æºé…ç½®ï¼‰â€”ç»“æœå˜é‡ï¼ˆå­¦ä¸šè¡¨ç°ã€åŠ³åŠ¨å¸‚åœºå›æŠ¥ï¼‰"

å·®è§„åˆ™ï¼ˆå¤ªæ³›ï¼Œç¦æ­¢ï¼‰: "æ•°å­—å¿…é¡»å¸¦å•ä½å’Œæ—¶é—´å£å¾„"
å¥½è§„åˆ™ï¼ˆå…·ä½“åˆ°ä¸»é¢˜ï¼‰: "æ¶‰åŠæ•°å­—æ—¶ï¼Œå¿…é¡»è¯´æ˜æ—¶é—´ã€åœ°åŒºã€æ ·æœ¬æˆ–æ”¿ç­–èƒŒæ™¯ç­‰å£å¾„ä¿¡æ¯ï¼›æ²¡æœ‰ç¡®åˆ‡æ•°å­—æ—¶é¿å…'ç¿»å€''å¤§å¹…'ç­‰æ¨¡ç³Šè¡¨è¿°"
"""
        
        # ä½¿ç”¨è‡ªåŠ¨é‡è¯•æœºåˆ¶
        response = await self._call_llm_with_retry(
            prompt=prompt,
            timeout=180.0,  # 3åˆ†é’Ÿ
            max_retries=2,  # è¿™ä¸ªæ­¥éª¤è¾ƒç®€å•ï¼Œ2æ¬¡é‡è¯•è¶³å¤Ÿ
            step_name="Step 0: ç”Ÿæˆé£æ ¼é…ç½®"
        )
        
        try:
            # å°è¯•è§£æJSON
            style_profile = json.loads(response)
        except:
            # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
            style_profile = {
                "enable_vox_intro": request.include_vox_intro,
                "main_structure": request.style_preference or "S1",
                "figure_style": "A",
                "rules": [
                    "åªä½¿ç”¨Evidence Ledgerä¸­å­˜åœ¨çš„äº‹å®",
                    "æ•°å­—å¿…é¡»å¸¦å•ä½ä¸æ—¶é—´å£å¾„",
                    "æ¯æ®µæœ«å°¾åŠ è¯æ®æ ‡æ³¨",
                    "å›¾è¡¨å¿…é¡»æ¥è‡ªæ–‡æ¡£",
                    "ç¦æ­¢å¤–æ¨å’Œè„‘è¡¥"
                ]
            }
        
        return style_profile
    
    async def _build_evidence_ledger(
        self,
        parsed_docs: List[Dict[str, Any]],
        topic_hint: str
    ) -> List[Dict[str, Any]]:
        """Step A: æ„å»ºæ–‡å­—è¯æ®å°è´¦"""
        
        evidence_ledger = []
        
        for doc in parsed_docs:
            doc_id = doc["doc_id"]
            title = doc["title"]
            
            # æå–æ®µè½æ–‡æœ¬
            paragraphs_text = "\n\n".join([
                f"[{p['paragraph_id']}] {p['text']}"
                for p in doc["paragraphs"]
                if p['type'] in ['paragraph', 'table_row']
            ])
            
            prompt = f"""
ä½ æ˜¯è¯æ®æå–ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹æ–‡æ¡£ä¸­æå–3-10æ¡æœ€å°äº‹å®å•å…ƒï¼ˆfindingsï¼‰ã€‚

æ–‡æ¡£æ ‡é¢˜: {title}
ä¸»é¢˜æç¤º: {topic_hint}

æ–‡æ¡£å†…å®¹:
{paragraphs_text[:8000]}  # é™åˆ¶é•¿åº¦

æ¯æ¡findingå¿…é¡»åŒ…å«:
- finding_index: åºå·ï¼ˆä»1å¼€å§‹ï¼‰
- type: ç±»å‹ï¼ˆç ”ç©¶å‘ç°/æ•°æ®æè¿°/ä½œè€…è§‚ç‚¹/æ”¿ç­–ä¿¡æ¯ï¼‰
- claim: äº‹å®é™ˆè¿°ï¼ˆä¸€å¥è¯ï¼Œå¿…é¡»åŸºäºåŸæ–‡ï¼‰
- numbers: æ¶‰åŠçš„æ•°å­—åˆ—è¡¨ï¼ˆå«å•ä½ï¼Œå¦‚["2020å¹´å¢é•¿15%", "æ ·æœ¬é‡1000äºº"]ï¼‰
- linked_assets: å…³è”çš„å›¾è¡¨IDåˆ—è¡¨ï¼ˆæš‚æ—¶ä¸ºç©ºï¼‰

è¯·ä»¥JSONæ•°ç»„æ ¼å¼è¾“å‡ºï¼Œä¾‹å¦‚:
[
    {{
        "finding_index": 1,
        "type": "æ•°æ®æè¿°",
        "claim": "2020å¹´ä¸­å›½GDPå¢é•¿2.3%",
        "numbers": ["2020å¹´", "2.3%"],
        "linked_assets": []
    }},
    ...
]

æ³¨æ„:
1. åªæå–åŸæ–‡æ˜ç¡®æåˆ°çš„äº‹å®
2. ç¦æ­¢æ¨æ–­ã€å¤–æ¨ã€å¸¸è¯†è¡¥å……
3. æ•°å­—å¿…é¡»å¸¦å•ä½å’Œæ—¶é—´å£å¾„
"""
            
            # ä½¿ç”¨è‡ªåŠ¨é‡è¯•æœºåˆ¶
            response = await self._call_llm_with_retry(
                prompt=prompt,
                timeout=180.0,  # 3åˆ†é’Ÿ
                max_retries=2,
                step_name=f"Step A: æå–æ–‡æ¡£ {doc_id} çš„è¯æ®"
            )
            
            try:
                findings = json.loads(response)
            except:
                # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•æå–JSONéƒ¨åˆ†
                json_match = re.search(r'\[.*\]', response, re.DOTALL)
                if json_match:
                    findings = json.loads(json_match.group())
                else:
                    findings = []
            
            # æ·»åŠ source_doc_id
            for finding in findings:
                finding["source_doc_id"] = doc_id
            
            evidence_ledger.append({
                "doc_id": doc_id,
                "title": title,
                "time_range": None,  # å¯ä»¥åç»­ä»æ–‡æ¡£ä¸­æå–
                "findings": findings
            })
        
        return evidence_ledger
    
    async def _build_visual_asset_ledger(
        self,
        parsed_docs: List[Dict[str, Any]],
        evidence_ledger: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Step A2: æ„å»ºå›¾è¡¨è¯æ®å°è´¦"""
        
        assets = []
        
        for doc in parsed_docs:
            doc_id = doc["doc_id"]
            logger.info(f"[Visual Assets] å¤„ç†æ–‡æ¡£ {doc_id}, imageså­—æ®µ: {'å­˜åœ¨' if 'images' in doc else 'ä¸å­˜åœ¨'}")
            if "images" in doc:
                logger.info(f"[Visual Assets] æ–‡æ¡£ {doc_id} åŒ…å« {len(doc['images'])} å¼ å›¾ç‰‡")
            
            # æå–è¡¨æ ¼
            table_paragraphs = [
                p for p in doc["paragraphs"]
                if p['type'] == 'table_row'
            ]
            
            if table_paragraphs:
                # æŒ‰è¡¨æ ¼åˆ†ç»„ï¼ˆç®€åŒ–å¤„ç†ï¼Œæ¯ä¸ªtable_rowä½œä¸ºä¸€ä¸ªè¡¨æ ¼ï¼‰
                for idx, table_para in enumerate(table_paragraphs[:5]):  # é™åˆ¶æœ€å¤š5ä¸ªè¡¨æ ¼
                    asset_id = f"{doc_id}-TAB-{idx + 1}"
                    
                    assets.append({
                        "asset_id": asset_id,
                        "asset_type": "TAB",
                        "original_label": None,
                        "caption_or_title": f"è¡¨æ ¼ {idx + 1}",
                        "location_anchor": table_para['text'][:100],
                        "key_numbers": self._extract_numbers_from_text(table_para['text']),
                        "takeaway_claim": None,
                        "linked_findings": [],
                        "editing_instruction": "çªå‡ºå…³é”®æ•°æ®åˆ—"
                    })
            
            # æå–å›¾ç‰‡
            if "images" in doc and doc["images"]:
                for idx, img in enumerate(doc["images"]):  # æå–æ‰€æœ‰å›¾ç‰‡
                    asset_id = f"{doc_id}-FIG-{idx + 1}"
                    
                    # å°è¯•ä»å‘¨å›´æ–‡æœ¬æ‰¾å›¾æ³¨
                    caption = self._find_caption_near_image(doc, idx)
                    
                    # è·å–å›¾ç‰‡URLå’Œå…¶ä»–ä¿¡æ¯
                    img_url = None
                    img_filename = None
                    img_path = None
                    
                    if isinstance(img, dict):
                        img_url = img.get("url")
                        img_filename = img.get("filename")
                        img_path = img.get("path")
                    
                    asset_data = {
                        "asset_id": asset_id,
                        "asset_type": "FIG",
                        "original_label": None,
                        "caption_or_title": caption or f"å›¾ {idx + 1}",
                        "location_anchor": caption[:100] if caption else None,
                        "key_numbers": [],
                        "takeaway_claim": None,
                        "linked_findings": [],
                        "editing_instruction": "çªå‡ºä¸»è¦è¶‹åŠ¿"
                    }
                    
                    # æ·»åŠ å›¾ç‰‡ç›¸å…³ä¿¡æ¯
                    logger.info(f"[Visual Assets] å¤„ç† {asset_id}, img æ•°æ®ç±»å‹: {type(img)}, å†…å®¹: {img}")
                    
                    if img_url:
                        asset_data["image_url"] = img_url
                        logger.info(f"[Visual Assets] âœ… {asset_id} - æ·»åŠ  image_url: {img_url}")
                    else:
                        logger.warning(f"[Visual Assets] âŒ {asset_id} - æ²¡æœ‰URLï¼imgæ•°æ®: {img}")
                    
                    if img_filename:
                        asset_data["image_filename"] = img_filename
                    if img_path:
                        asset_data["image_path"] = img_path
                    
                    logger.info(f"[Visual Assets] {asset_id} æœ€ç»ˆ asset_data åŒ…å«å­—æ®µ: {list(asset_data.keys())}")
                    assets.append(asset_data)
        
        logger.info(f"[Visual Assets] æ€»å…±ç”Ÿæˆ {len(assets)} ä¸ªè§†è§‰èµ„äº§")
        for asset in assets:
            if asset["asset_type"] == "FIG":
                has_url = "image_url" in asset
                logger.info(f"[Visual Assets] {asset['asset_id']}: {asset['caption_or_title']} - URL: {'æœ‰' if has_url else 'æ— '}")
        
        return {
            "assets": assets
        }
    
    def _extract_numbers_from_text(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–æ•°å­—ï¼ˆå¸¦å•ä½ï¼‰"""
        # åŒ¹é…æ•°å­—+å•ä½çš„æ¨¡å¼
        patterns = [
            r'\d+\.?\d*%',  # ç™¾åˆ†æ¯”
            r'\d+\.?\d*[äº¿ä¸‡åƒç™¾å]?[å…ƒäººæ¬¡ä¸ª]',  # ä¸­æ–‡å•ä½
            r'\d{4}å¹´',  # å¹´ä»½
            r'\d+\.?\d*\s*[a-zA-Z]+',  # è‹±æ–‡å•ä½
        ]
        
        numbers = []
        for pattern in patterns:
            matches = re.findall(pattern, text)
            numbers.extend(matches)
        
        return numbers[:8]  # æœ€å¤šè¿”å›8ä¸ª
    
    def _find_caption_near_image(self, doc: Dict[str, Any], img_idx: int) -> Optional[str]:
        """åœ¨å›¾ç‰‡é™„è¿‘æŸ¥æ‰¾å›¾æ³¨"""
        # æŸ¥æ‰¾åŒ…å«"å›¾"å­—çš„æ®µè½ï¼Œå¹¶å°è¯•æå–æ›´å‡†ç¡®çš„æ ‡é¢˜
        for para in doc["paragraphs"]:
            text = para.get("text", "")
            # åŒ¹é… "å›¾Xï¼šæ ‡é¢˜" æˆ– "å›¾X æ ‡é¢˜" æˆ– "Figure X: æ ‡é¢˜" ç­‰æ ¼å¼
            import re
            patterns = [
                r'å›¾\s*\d+[ï¼š:]\s*(.+)',  # å›¾1ï¼šæ ‡é¢˜
                r'å›¾\s*\d+[\s\.]+(.+)',   # å›¾1 æ ‡é¢˜ æˆ– å›¾1. æ ‡é¢˜
                r'Figure\s+\d+[ï¼š:]\s*(.+)',  # Figure 1: æ ‡é¢˜
                r'Fig\.\s*\d+[ï¼š:]\s*(.+)',   # Fig. 1: æ ‡é¢˜
            ]
            
            for pattern in patterns:
                match = re.search(pattern, text, re.IGNORECASE)
                if match:
                    caption = match.group(1).strip()
                    # é™åˆ¶é•¿åº¦ï¼Œå»æ‰è¿‡é•¿çš„æè¿°
                    if len(caption) > 100:
                        caption = caption[:100] + "..."
                    return caption
            
            # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ç‰¹å®šæ ¼å¼ï¼Œä½†åŒ…å«"å›¾"å­—ä¸”é•¿åº¦åˆé€‚
            if ("å›¾" in text or "Figure" in text or "Fig" in text) and 10 < len(text) < 200:
                # æ¸…ç†æ–‡æœ¬
                caption = text.strip()
                if len(caption) > 100:
                    caption = caption[:100] + "..."
                return caption
        
        return None
    
    async def _select_structure(
        self,
        topic_hint: str,
        evidence_ledger: List[Dict[str, Any]],
        visual_asset_ledger: Dict[str, Any],
        style_profile: Dict[str, Any],
        style_preference: Optional[str]
    ) -> Dict[str, Any]:
        """Step B: é€‰æ‹©ç»“æ„"""
        
        # å¦‚æœç”¨æˆ·æŒ‡å®šäº†ç»“æ„ï¼Œç›´æ¥ä½¿ç”¨
        if style_preference and style_preference in ["S1", "S2", "S3", "S4"]:
            main_structure = style_preference
        else:
            main_structure = style_profile.get("main_structure", "S1")
        
        # æ„å»ºè¯æ®æ‘˜è¦
        evidence_summary = "\n".join([
            f"æ–‡æ¡£{ledger['doc_id']}: {len(ledger['findings'])}æ¡è¯æ®"
            for ledger in evidence_ledger
        ])
        
        prompt = f"""
ä½ æ˜¯å£æ’­ç¨¿ç»“æ„è®¾è®¡ä¸“å®¶ã€‚è¯·ä¸ºä»¥ä¸‹ä¸»é¢˜è®¾è®¡å£æ’­ç¨¿ç»“æ„ã€‚

ä¸»é¢˜: {topic_hint}
é€‰å®šç»“æ„: {main_structure}
è¯æ®æ¦‚è§ˆ:
{evidence_summary}

è¯·è¾“å‡º3-6ä¸ªä¸€çº§å°æ ‡é¢˜ï¼ˆæ¯ä¸ªâ‰¤12å­—ï¼‰ï¼Œå¹¶ä¸ºæ¯ä¸ªå°æ ‡é¢˜åˆ†é…ç›¸å…³æ–‡æ¡£å’Œå›¾è¡¨ã€‚

è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰:
{{
    "structure_type": "{main_structure}",
    "sections": [
        {{
            "section_id": 1,
            "title": "å°æ ‡é¢˜1",
            "related_docs": ["D1", "D2"],
            "related_assets": ["D1-FIG-1", "D2-TAB-1"]
        }},
        ...
    ]
}}
"""
        
        # ä½¿ç”¨è‡ªåŠ¨é‡è¯•æœºåˆ¶
        response = await self._call_llm_with_retry(
            prompt=prompt,
            timeout=180.0,  # 3åˆ†é’Ÿ
            max_retries=2,
            step_name="Step B: é€‰æ‹©ç»“æ„"
        )
        
        try:
            structure = json.loads(response)
        except:
            # é»˜è®¤ç»“æ„
            structure = {
                "structure_type": main_structure,
                "sections": [
                    {"section_id": 1, "title": "å¼•è¨€", "related_docs": [], "related_assets": []},
                    {"section_id": 2, "title": "ä¸»ä½“", "related_docs": [], "related_assets": []},
                    {"section_id": 3, "title": "ç»“è®º", "related_docs": [], "related_assets": []}
                ]
            }
        
        return structure
    
    async def _generate_script_review(
        self,
        request: IntegratedVoiceoverRequest,
        style_profile: Dict[str, Any],
        evidence_ledger: List[Dict[str, Any]],
        visual_asset_ledger: Dict[str, Any],
        structure: Dict[str, Any]
    ) -> str:
        """Step C: ç”Ÿæˆå®¡é˜…ç‰ˆå£æ’­ç¨¿ï¼ˆå¸¦è¯æ®æ ‡æ³¨ï¼‰"""
        
        # æ„å»ºå®Œæ•´çš„è¯æ®å’Œå›¾è¡¨ä¿¡æ¯
        evidence_text = json.dumps(evidence_ledger, ensure_ascii=False, indent=2)
        assets_text = json.dumps(visual_asset_ledger, ensure_ascii=False, indent=2)
        structure_text = json.dumps(structure, ensure_ascii=False, indent=2)
        
        # æ„å»ºç‰‡å¤´
        intro_text = ""
        if style_profile.get("enable_vox_intro", True):
            speaker_aff = request.speaker_affiliation or "VoxChina"
            speaker_name = request.speaker_name or "ä¸»æ’­"
            intro_text = f"""
å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯{speaker_aff}çš„{speaker_name}ã€‚
å¾ˆé«˜å…´åœ¨VOXCHINAå’Œå¤§å®¶è§é¢ã€‚
"""
        
        # å­—æ•°é™åˆ¶è¯´æ˜
        word_limit_instruction = ""
        if request.word_limit and request.word_limit > 0:
            word_limit_instruction = f"""
ã€å­—æ•°è¦æ±‚ã€‘
è¯·å°†å£æ’­ç¨¿æ€»å­—æ•°æ§åˆ¶åœ¨ {request.word_limit} å­—å·¦å³ï¼ˆå…è®¸Â±10%æµ®åŠ¨ï¼‰ã€‚
åœ¨ä¿è¯å†…å®¹å®Œæ•´æ€§çš„å‰æä¸‹ï¼Œç²¾ç®€è¡¨è¾¾ï¼Œçªå‡ºé‡ç‚¹ã€‚
"""
        
        prompt = f"""
ä½ æ˜¯VoxChinaå£æ’­ç¨¿æ’°å†™ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹ææ–™ï¼Œç”Ÿæˆä¸€ä»½å®Œæ•´çš„å£æ’­ç¨¿ï¼ˆå®¡é˜…ç‰ˆï¼Œå¸¦è¯æ®æ ‡æ³¨ï¼‰ã€‚

ã€ä¸»é¢˜ã€‘
{request.topic_hint}

ã€é£æ ¼é…ç½®ã€‘
{json.dumps(style_profile, ensure_ascii=False, indent=2)}

ã€è¯æ®å°è´¦ã€‘
{evidence_text[:10000]}

ã€å›¾è¡¨å°è´¦ã€‘
{assets_text[:5000]}

ã€ç»“æ„è®¾è®¡ã€‘
{structure_text}
{word_limit_instruction}
ã€è¾“å‡ºè¦æ±‚ã€‘
è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºå®Œæ•´å£æ’­ç¨¿:

## æ ‡é¢˜
ï¼ˆâ‰¤20å­—ï¼Œå¸å¼•äººçš„æ ‡é¢˜ï¼‰

## ç‰‡å¤´
{intro_text}

## ç‚¹é¢˜æ®µ
ï¼ˆ1æ®µï¼Œè¯´æ˜ç°å®å†²å‡»/çŸ›ç›¾ + æœ¬æœŸç»“æ„è¯´æ˜ï¼Œå¿…é¡»æœ‰è¯æ®æ”¯æ’‘ï¼‰
ã€è¯æ®ï¼šæ¥æºæ–‡æ¡£ID - findingç¼–å·ã€‘

## æ­£æ–‡
### å°æ ‡é¢˜1
æ®µè½1å†…å®¹...
ã€è¯æ®ï¼šD1-F1ã€‘
ã€å›¾è¡¨ï¼šD1-FIG-1ã€‘

æ®µè½2å†…å®¹...
ã€è¯æ®ï¼šD1-F2, D2-F1ã€‘

### å°æ ‡é¢˜2
...

## ç»“å°¾
ï¼ˆå›æ‰£ç»“æ„ + æ”¶æŸè¯­ï¼‰
æ„Ÿè°¢å¤§å®¶è§‚çœ‹æœ¬æœŸè§†é¢‘ï¼Œæ¬¢è¿ç»§ç»­å…³æ³¨ VOXCHINAï¼Œæˆ‘ä»¬ä¸‹æœŸå†è§ï¼

ã€ç¡¬æ€§è§„åˆ™ã€‘
R1: åªä½¿ç”¨Evidence Ledgerä¸­å­˜åœ¨çš„äº‹å®ï¼Œç¦æ­¢å¤–æ¨
R2: æ•°å­—å¿…é¡»å¸¦å•ä½ä¸æ—¶é—´å£å¾„
R3: æ¯æ®µæœ«å°¾å¿…é¡»åŠ ã€è¯æ®ï¼š...ã€‘æ ‡æ³¨
R4: å›¾è¡¨å¼•ç”¨å¿…é¡»æ¥è‡ªVisual Asset Ledgerï¼Œå¹¶ä¸¥æ ¼ä½¿ç”¨ã€å›¾è¡¨ï¼šasset_idã€‘æ ¼å¼ï¼ˆå¦‚ã€å›¾è¡¨ï¼šD1-FIG-1ã€‘ï¼‰ã€‚ç¦æ­¢ä½¿ç”¨ã€”ç”»é¢ï¼š...ã€•ã€[å›¾ï¼š...]ç­‰å…¶ä»–æ ¼å¼
R5: è¯­æ°”å¤§ä¼—å¯æ‡‚ä½†ä¸“ä¸šå…‹åˆ¶ï¼Œå¤šçŸ­å¥
R6: æ¯ä¸ªæ­£æ–‡å°èŠ‚è‡³å°‘å¼•ç”¨ä¸€ä¸ªç›¸å…³å›¾è¡¨ï¼ˆå¦‚æœVisual Asset Ledgerä¸­å­˜åœ¨ç›¸å…³å›¾è¡¨çš„è¯ï¼‰

è¯·å¼€å§‹æ’°å†™:
"""
        
        # ä½¿ç”¨è‡ªåŠ¨é‡è¯•æœºåˆ¶è°ƒç”¨LLM
        script_review = await self._call_llm_with_retry(
            prompt=prompt,
            timeout=300.0,  # 5åˆ†é’Ÿè¶…æ—¶
            max_retries=3,
            step_name="Step C: ç”Ÿæˆå®¡é˜…ç‰ˆå£æ’­ç¨¿"
        )
        
        return script_review
    
    async def _generate_script_final(self, script_review: str) -> str:
        """Step D: ç”Ÿæˆä¸Šå±ç‰ˆå£æ’­ç¨¿ï¼ˆåˆ é™¤è¯æ®æ ‡æ³¨ï¼Œä¿ç•™å›¾è¡¨æ ‡è®°ä¾›å‰ç«¯æ¸²æŸ“ï¼‰"""
        
        script_final = script_review
        
        # åˆ é™¤æ‰€æœ‰è¯æ®æ ‡æ³¨ï¼ˆè¦†ç›–å…¨è§’ã€åŠè§’ã€åœ†æ‹¬å·ç­‰å¤šç§å˜ä½“ï¼‰
        # å…¨è§’æ–¹æ‹¬å·ï¼šã€è¯æ®ï¼š...ã€‘
        script_final = re.sub(r'ã€è¯æ®[ï¼š:]([^ã€‘]+)ã€‘', '', script_final)
        # åŠè§’æ–¹æ‹¬å·ï¼š[è¯æ®ï¼š...] æˆ– [è¯æ®:...]
        script_final = re.sub(r'\[è¯æ®[ï¼š:]([^\]]+)\]', '', script_final)
        # åœ†æ‹¬å·å˜ä½“ï¼šï¼ˆè¯æ®ï¼š...ï¼‰
        script_final = re.sub(r'ï¼ˆè¯æ®[ï¼š:]([^ï¼‰]+)ï¼‰', '', script_final)
        # è‹±æ–‡æ ¼å¼å˜ä½“ï¼š[Evidence: ...] æˆ– ã€Evidence: ...ã€‘
        script_final = re.sub(r'[\[ã€]Evidence[ï¼š:][^\]ã€‘]+[\]ã€‘]', '', script_final, flags=re.IGNORECASE)
        
        # æ¸…ç†è¡Œæœ«å¤šä½™ç©ºæ ¼ï¼ˆè¯æ®æ ‡æ³¨åˆ é™¤åå¯èƒ½ç•™ä¸‹çš„ï¼‰
        script_final = re.sub(r' +$', '', script_final, flags=re.MULTILINE)
        
        # æ¸…ç†å¤šä½™ç©ºè¡Œ
        script_final = re.sub(r'\n{3,}', '\n\n', script_final)
        
        return script_final.strip()
    
    def get_task_status(self, task_id: str) -> Optional[Dict[str, Any]]:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        return self.tasks.get(task_id)
    
    def get_task_result(self, task_id: str) -> Optional[IntegratedVoiceoverResponse]:
        """è·å–ä»»åŠ¡ç»“æœ"""
        task_data = self.tasks.get(task_id)
        if not task_data:
            logger.warning(f"Task {task_id} not found in tasks")
            return None
        
        try:
            # è°ƒè¯•æ—¥å¿—
            logger.info(f"Getting task result for {task_id}, status: {task_data.get('status')}")
            
            # ç»Ÿä¸€è½¬æ¢æ—¥æœŸæ ¼å¼ä¸ºå­—ç¬¦ä¸²ï¼ˆå¦‚æœæ˜¯ datetime å¯¹è±¡ï¼‰
            created_at = task_data["created_at"]
            if isinstance(created_at, datetime):
                created_at = created_at.isoformat()
            
            updated_at = task_data["updated_at"]
            if isinstance(updated_at, datetime):
                updated_at = updated_at.isoformat()
            
            # æ„å»ºå“åº”å¯¹è±¡
            response = IntegratedVoiceoverResponse(
                task_id=task_data["task_id"],
                status=task_data["status"],
                style_profile=StyleProfile(**task_data["style_profile"]) if task_data.get("style_profile") else None,
                evidence_ledger=[EvidenceLedger(**ledger) for ledger in task_data.get("evidence_ledger", [])] if task_data.get("evidence_ledger") else None,
                visual_asset_ledger=VisualAssetLedger(**task_data["visual_asset_ledger"]) if task_data.get("visual_asset_ledger") else None,
                structure=task_data.get("structure"),
                script_review=task_data.get("script_review"),
                script_final=task_data.get("script_final"),
                created_at=created_at,
                updated_at=updated_at
            )
            
            logger.info(f"âœ… Task result built successfully for {task_id}")
            return response
            
        except Exception as e:
            logger.error(f"âŒ Error building task result for {task_id}: {e}", exc_info=True)
            logger.error(f"Task data keys: {task_data.keys()}")
            if task_data.get("visual_asset_ledger"):
                logger.error(f"Visual asset ledger: {task_data['visual_asset_ledger']}")
            raise


# å•ä¾‹
integrated_voiceover_service = IntegratedVoiceoverService()
