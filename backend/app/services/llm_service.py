import httpx
import json
import os
from loguru import logger
from app.core.config import settings
from openai import AsyncOpenAI

PROMPTS = {
    "zh": {
        "summary": """
        è¯·ä¸ºä»¥ä¸‹æ–‡ç« ç”Ÿæˆä¸€ä¸ªç²¾ç‚¼çš„æ‘˜è¦ï¼Œå­—æ•°æŽ§åˆ¶åœ¨{max_words}å­—ä»¥å†…ã€‚
        æ‘˜è¦åº”ä¿ç•™æ ¸å¿ƒè§‚ç‚¹ï¼Œé€‚åˆä½œä¸ºéŸ³é¢‘æ’­æŠ¥çš„æ–‡æ¡ˆã€‚
        
        æ–‡ç« å†…å®¹ï¼š
        {content}
        """,
        "integrate": """
        è¯·é˜…è¯»ä»¥ä¸‹å¤šç¯‡æ–‡ç« çš„å†…å®¹ï¼Œå¹¶å°†å®ƒä»¬æ•´åˆæˆä¸€ç¯‡è¿žè´¯ã€æœ‰æ·±åº¦çš„æ–‡ç« ã€‚
        
        è¦æ±‚ï¼š
        1. å­—æ•°æŽ§åˆ¶åœ¨ {max_words} å­—å·¦å³ã€‚
        2. {focus_prompt}
        3. è¯­è¨€æµç•…ï¼Œé€»è¾‘æ¸…æ™°ï¼Œé¿å…ç®€å•çš„æ‹¼æŽ¥ã€‚
        4. é€‚åˆä½œä¸ºéŸ³é¢‘æ’­æŠ¥çš„æ–‡æ¡ˆã€‚
        
        æ–‡ç« å†…å®¹ï¼š
        {combined_content}
        """,
        "focus_default": "ç»¼åˆæ‰€æœ‰æ–‡ç« çš„å…³é”®ç‚¹ã€‚",
        "focus_prefix": "é‡ç‚¹å…³æ³¨ï¼š",
        "answer": """
        è¯·æ ¹æ®æä¾›çš„å‚è€ƒèµ„æ–™å›žç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
        å¦‚æžœå‚è€ƒèµ„æ–™ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯šå®žåœ°è¯´æ˜Žæ— æ³•å›žç­”ã€‚
        
        å‚è€ƒèµ„æ–™ï¼š
        {context_block}
        
        ç”¨æˆ·é—®é¢˜ï¼š
        {query}
        
        è¯·ç”¨ä¸“ä¸šã€è¿žè´¯çš„è¯­è¨€å›žç­”ï¼š
        """,
        "trending": """
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¤¾äº¤åª’ä½“è¿è¥ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ï¼Œé’ˆå¯¹è¯é¢˜â€œ{topic}â€å†™ä¸€ç¯‡å¿«é€Ÿå“åº”æŽ¨æ–‡ã€‚
        
        æœ€æ–°ç½‘ç»œèµ„è®¯ï¼š
        {web_text}
        
        ç›¸å…³èƒŒæ™¯çŸ¥è¯†ï¼ˆè‡ªæœ‰åº“ï¼‰ï¼š
        {local_text}
        
        è¦æ±‚ï¼š
        1. æ ‡é¢˜å¸å¼•çœ¼çƒã€‚
        2. å†…å®¹ç»“åˆæ—¶äº‹çƒ­ç‚¹å’Œæ·±åº¦èƒŒæ™¯ã€‚
        3. è¯­æ°”ä¸“ä¸šã€å®¢è§‚ä½†æœ‰è§‚ç‚¹ã€‚
        4. é€‚åˆå¾®ä¿¡å…¬ä¼—å·æˆ–ä»Šæ—¥å¤´æ¡å‘å¸ƒã€‚
        """,
        "script": """
        è¯·å°†ä»¥ä¸‹æ–‡ç« å†…å®¹æ”¹ç¼–ä¸ºçŸ­è§†é¢‘åˆ†é•œè„šæœ¬ã€‚
        
        æ–‡ç« å†…å®¹ï¼š
        {content}
        
        è¾“å‡ºæ ¼å¼è¦æ±‚ï¼ˆMarkdownè¡¨æ ¼ï¼‰ï¼š
        | é•œå· | ç”»é¢æè¿° (Visual) | å£æ’­æ–‡æ¡ˆ (Audio) | å¤‡æ³¨ |
        |---|---|---|---|
        | 1 | [åœºæ™¯æè¿°] | [å¯¹åº”å°è¯] | [éŸ³æ•ˆ/è´´å›¾å»ºè®®] |
        
        è¦æ±‚ï¼š
        1. èŠ‚å¥ç´§å‡‘ï¼Œé€‚åˆ1-3åˆ†é’ŸçŸ­è§†é¢‘ã€‚
        2. ç”»é¢æè¿°è¦å…·ä½“ï¼ŒåŒ…å«è¿é•œå»ºè®®ã€‚
        3. å£æ’­æ–‡æ¡ˆè¦å£è¯­åŒ–ã€‚
        """
    },
    "en": {
        "summary": """
        Please generate a concise summary for the following article, keeping it under {max_words} words.
        The summary should retain core points and be suitable for audio broadcasting.
        
        Article Content:
        {content}
        """,
        "integrate": """
        Please read the contents of the following multiple articles and integrate them into a coherent, in-depth article.
        
        Requirements:
        1. Keep the word count around {max_words} words.
        2. {focus_prompt}
        3. Ensure fluent language and clear logic; avoid simple splicing.
        4. Suitable for audio broadcasting.
        
        Article Contents:
        {combined_content}
        """,
        "focus_default": "Synthesize the key points from all articles.",
        "focus_prefix": "Focus on: ",
        "answer": """
        Please answer the user's question based on the provided reference materials.
        If there is no relevant information in the references, honestly state that you cannot answer.
        
        Reference Materials:
        {context_block}
        
        User Question:
        {query}
        
        Please answer in professional and coherent language:
        """,
        "trending": """
        You are a professional social media operations expert. Please write a quick response post for the topic "{topic}" based on the following information.
        
        Latest Web Info:
        {web_text}
        
        Relevant Background (Local Knowledge Base):
        {local_text}
        
        Requirements:
        1. Catchy title.
        2. Combine current events with deep background.
        3. Professional, objective tone but with a viewpoint.
        4. Suitable for posting on Twitter or LinkedIn.
        """,
        "script": """
        Please adapt the following article content into a short video storyboard script.
        
        Article Content:
        {content}
        
        Output Format Requirement (Markdown Table):
        | Shot | Visual Description | Audio Script | Notes |
        |---|---|---|---|
        | 1 | [Scene Description] | [Lines] | [SFX/Sticker Suggestions] |
        
        Requirements:
        1. Fast-paced, suitable for a 1-3 minute short video.
        2. Visual descriptions should be specific, including camera movement suggestions.
        3. Audio script should be colloquial.
        """
    }
}

class LLMService:
    def __init__(self):
        self.provider = settings.LLM_PROVIDER
        self.model = settings.LLM_MODEL
        self.config_file = "llm_config.json"
        
        # Load persisted config if exists
        self._load_config_from_file()
        
        # Ollama config
        self.ollama_base_url = settings.OLLAMA_HOST
        self.ollama_model = settings.OLLAMA_MODEL
        
        # OpenAI config
        if self.provider == "openai":
            if not settings.OPENAI_API_KEY:
                logger.error("âš ï¸  OPENAI_API_KEY æœªé…ç½®ï¼è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®æˆ–é€šè¿‡ç®¡ç†ç•Œé¢é…ç½®")
            self.openai_client = AsyncOpenAI(
                api_key=settings.OPENAI_API_KEY or "dummy-key",  # é˜²æ­¢åˆå§‹åŒ–å¤±è´¥
                base_url=settings.OPENAI_API_BASE,
                timeout=300.0  # 5åˆ†é’Ÿè¶…æ—¶ï¼Œé€‚é…gpt-4oçš„é•¿å“åº”æ—¶é—´
            )
        else:
            self.openai_client = None

    def _load_config_from_file(self):
        """Load configuration from local file (including API key)."""
        try:
            if os.path.exists(self.config_file):
                with open(self.config_file, 'r') as f:
                    config = json.load(f)
                    if config.get('provider'):
                        self.provider = config['provider']
                        settings.LLM_PROVIDER = config['provider']
                    if config.get('model'):
                        self.model = config['model']
                        settings.LLM_MODEL = config['model']
                    # Load API key if exists
                    if config.get('api_key') and config.get('provider') == 'openai':
                        settings.OPENAI_API_KEY = config['api_key']
                        logger.info("âœ… å·²ä»Žé…ç½®æ–‡ä»¶åŠ è½½ API Key")
                        settings.LLM_MODEL = config['model']
                logger.info(f"Loaded LLM config from file: {self.provider}/{self.model}")
        except Exception as e:
            logger.error(f"Failed to load LLM config file: {e}")

    def _save_config_to_file(self):
        """Save configuration to local file (including API key)."""
        try:
            config = {
                "provider": self.provider,
                "model": self.model
            }
            # Save API key if set (for openai provider)
            if self.provider == "openai" and settings.OPENAI_API_KEY:
                config["api_key"] = settings.OPENAI_API_KEY
            
            with open(self.config_file, 'w') as f:
                json.dump(config, f, indent=2)
            logger.info(f"âœ… å·²ä¿å­˜ LLM é…ç½®åˆ°æ–‡ä»¶: {self.config_file}")
        except Exception as e:
            logger.error(f"Failed to save LLM config file: {e}")

    def _get_prompt(self, key: str, language: str = "zh") -> str:
        lang = language if language in ["zh", "en"] else "zh"
        return PROMPTS[lang][key]
    
    async def _generate_with_provider(self, prompt: str, timeout: float = 60.0, model: str = None, system_prompt: str = None) -> str:
        """
        Generate text using the configured LLM provider (OpenAI or Ollama).
        
        Args:
            prompt: User prompt
            timeout: Request timeout in seconds
            model: Optional model override
            system_prompt: Optional system prompt (defaults to generic assistant prompt)
        """
        if self.provider == "openai":
            # æ£€æŸ¥ API Key
            if not settings.OPENAI_API_KEY:
                raise Exception("âŒ LLM API Key æœªé…ç½®ï¼è¯·è”ç³»ç®¡ç†å‘˜åœ¨ã€LLMè®¾ç½®ã€‘ä¸­é…ç½® API å¯†é’¥")
            
            try:
                # Use custom system prompt if provided, otherwise use default
                sys_content = system_prompt if system_prompt else "You are a helpful AI assistant that provides professional and accurate responses."
                
                response = await self.openai_client.chat.completions.create(
                    model=model or self.model,
                    messages=[
                        {"role": "system", "content": sys_content},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.7,
                    timeout=timeout
                )
                return response.choices[0].message.content.strip()
            except Exception as e:
                error_msg = str(e)
                logger.error(f"Error generating with OpenAI: {error_msg}")
                
                # æ£€æŸ¥å…·ä½“é”™è¯¯ç±»åž‹
                if "timeout" in error_msg.lower() or "timed out" in error_msg.lower():
                    raise Exception("LLMå“åº”è¶…æ—¶ï¼Œè¯·ç¨åŽé‡è¯•")
                elif "rate limit" in error_msg.lower() or "429" in error_msg:
                    raise Exception("APIè°ƒç”¨é¢‘çŽ‡é™åˆ¶ï¼Œè¯·ç¨åŽé‡è¯•")
                elif "401" in error_msg or "unauthorized" in error_msg.lower():
                    raise Exception("LLM APIè®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
                elif "connection" in error_msg.lower() or "connect" in error_msg.lower():
                    raise Exception(f"æ— æ³•è¿žæŽ¥åˆ°LLMæœåŠ¡: {error_msg}")
                else:
                    raise Exception(f"LLMå¤„ç†å¤±è´¥: {error_msg[:200]}")
        else:  # ollama
            try:
                # For Ollama, prepend system prompt to user prompt if provided
                full_prompt = prompt
                if system_prompt:
                    full_prompt = f"{system_prompt}\n\n{prompt}"
                
                async with httpx.AsyncClient() as client:
                    response = await client.post(
                        f"{self.ollama_base_url}/api/generate",
                        json={
                            "model": self.ollama_model,
                            "prompt": full_prompt,
                            "stream": False
                        },
                        timeout=timeout
                    )
                    response.raise_for_status()
                    result = response.json()
                    return result.get("response", "").strip()
            except Exception as e:
                logger.error(f"Error generating with Ollama: {str(e)}")
                raise Exception("Failed to generate with Ollama")

    async def generate_summary(self, content: str, max_words: int = 200, language: str = "zh") -> str:
        """
        Generates a summary of the provided content using configured LLM.
        """
        prompt_template = self._get_prompt("summary", language)
        prompt = prompt_template.format(max_words=max_words, content=content)
        return await self._generate_with_provider(prompt, timeout=60.0)

    async def integrate_articles(self, contents: list[str], max_words: int = 500, focus: str = None, language: str = "zh") -> str:
        """
        Integrates multiple articles into a single coherent narrative.
        """
        combined_content = "\n\n--- ARTICLE SEPARATOR ---\n\n".join(contents)
        
        lang_config = PROMPTS.get(language, PROMPTS["zh"])
        focus_default = lang_config["focus_default"]
        focus_prefix = lang_config["focus_prefix"]
        
        focus_prompt = f"{focus_prefix}{focus}" if focus else focus_default
        
        prompt_template = self._get_prompt("integrate", language)
        prompt = prompt_template.format(max_words=max_words, focus_prompt=focus_prompt, combined_content=combined_content)
        return await self._generate_with_provider(prompt, timeout=120.0)

    async def answer_query(self, query: str, context_texts: list[str], language: str = "zh") -> str:
        """
        Answers a user query based on the provided context (RAG).
        """
        # Combine context
        context_block = "\n\n---\n\n".join(context_texts)
        
        prompt_template = self._get_prompt("answer", language)
        prompt = prompt_template.format(context_block=context_block, query=query)
        return await self._generate_with_provider(prompt, timeout=60.0)

    async def generate_trending_post(self, topic: str, web_context: list[dict], local_context: list[str], language: str = "zh") -> str:
        """
        Generates a trending response post combining web and local info.
        """
        web_text = "\n".join([f"- {r.get('title')}: {r.get('body')}" for r in web_context])
        local_text = "\n".join(local_context)
        
        prompt_template = self._get_prompt("trending", language)
        prompt = prompt_template.format(topic=topic, web_text=web_text, local_text=local_text)
        return await self._generate_with_provider(prompt, timeout=60.0)

    async def generate_video_script(self, content: str, language: str = "zh") -> str:
        """
        Generates a video script with visual cues based on content.
        """
        prompt_template = self._get_prompt("script", language)
        prompt = prompt_template.format(content=content)
        return await self._generate_with_provider(prompt, timeout=90.0)
    
    async def generate_completion(self, prompt: str, system_prompt: str = None, model: str = None, timeout: float = 60.0) -> str:
        """
        é€šç”¨çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ï¼Œç”¨äºŽè‡ªå®šä¹‰prompt
        """
        return await self._generate_with_provider(prompt, timeout=timeout, model=model, system_prompt=system_prompt)
    
    async def fetch_available_models(self) -> list[dict]:
        """
        Fetch available models from the configured LLM provider.
        """
        if self.provider == "openai":
            try:
                # Check if API key is configured
                if not settings.OPENAI_API_KEY:
                    logger.warning("âš ï¸  API Key æœªé…ç½®ï¼Œè¿”å›žé»˜è®¤æ¨¡åž‹åˆ—è¡¨")
                    raise Exception("API Key not configured")
                
                # Use a longer timeout for listing models
                logger.info("ðŸ”„ æ­£åœ¨ä»Ž API èŽ·å–æ¨¡åž‹åˆ—è¡¨...")
                models = await self.openai_client.models.list(timeout=15.0)
                
                # èŽ·å–æ‰€æœ‰æ¨¡åž‹ï¼Œä¸è¿›è¡Œè¿‡æ»¤
                model_list = []
                logger.info(f"ðŸ“Š API è¿”å›žäº† {len(models.data)} ä¸ªæ¨¡åž‹")
                
                for model in models.data:
                    model_list.append({
                        "id": model.id,
                        "name": model.id,
                        "provider": "openai"
                    })
                    logger.debug(f"   - {model.id}")
                
                # æŒ‰åç§°æŽ’åº
                model_list = sorted(model_list, key=lambda x: x["id"])
                logger.info(f"âœ… æˆåŠŸèŽ·å–å¹¶è¿”å›ž {len(model_list)} ä¸ªæ¨¡åž‹")
                return model_list
                
            except Exception as e:
                logger.error(f"âŒ èŽ·å–æ¨¡åž‹åˆ—è¡¨å¤±è´¥: {str(e)}")
                logger.info("ðŸ“‹ è¿”å›žé»˜è®¤æ¨¡åž‹åˆ—è¡¨ï¼ˆ18ä¸ªï¼‰")
                # Return Comprehensive default models if API call fails
                return [
                    {"id": "gpt-4o", "name": "gpt-4o", "provider": "openai"},
                    {"id": "gpt-4o-2024-05-13", "name": "gpt-4o-2024-05-13", "provider": "openai"},
                    {"id": "gpt-4o-mini", "name": "gpt-4o-mini", "provider": "openai"},
                    {"id": "gpt-4o-mini-2024-07-18", "name": "gpt-4o-mini-2024-07-18", "provider": "openai"},
                    {"id": "gpt-4-turbo", "name": "gpt-4-turbo", "provider": "openai"},
                    {"id": "gpt-4-turbo-2024-04-09", "name": "gpt-4-turbo-2024-04-09", "provider": "openai"},
                    {"id": "gpt-4-turbo-preview", "name": "gpt-4-turbo-preview", "provider": "openai"},
                    {"id": "gpt-4-0125-preview", "name": "gpt-4-0125-preview", "provider": "openai"},
                    {"id": "gpt-4-1106-preview", "name": "gpt-4-1106-preview", "provider": "openai"},
                    {"id": "gpt-4", "name": "gpt-4", "provider": "openai"},
                    {"id": "gpt-4-0613", "name": "gpt-4-0613", "provider": "openai"},
                    {"id": "gpt-3.5-turbo", "name": "gpt-3.5-turbo", "provider": "openai"},
                    {"id": "gpt-3.5-turbo-0125", "name": "gpt-3.5-turbo-0125", "provider": "openai"},
                    {"id": "gpt-3.5-turbo-1106", "name": "gpt-3.5-turbo-1106", "provider": "openai"},
                    {"id": "o1-preview", "name": "o1-preview", "provider": "openai"},
                    {"id": "o1-preview-2024-09-12", "name": "o1-preview-2024-09-12", "provider": "openai"},
                    {"id": "o1-mini", "name": "o1-mini", "provider": "openai"},
                    {"id": "o1-mini-2024-09-12", "name": "o1-mini-2024-09-12", "provider": "openai"}
                ]
        else:  # ollama
            try:
                async with httpx.AsyncClient() as client:
                    response = await client.get(f"{self.ollama_base_url}/api/tags", timeout=5.0)
                    response.raise_for_status()
                    data = response.json()
                    models = data.get("models", [])
                    return [{"id": m["name"], "name": m["name"], "provider": "ollama"} for m in models]
            except Exception as e:
                logger.error(f"Error fetching Ollama models: {str(e)}")
                return []
    
    def update_config(self, provider: str = None, model: str = None, api_key: str = None):
        """
        Update LLM configuration at runtime.
        """
        if provider:
            self.provider = provider
            settings.LLM_PROVIDER = provider
        
        if model:
            self.model = model
            settings.LLM_MODEL = model
        
        if api_key and provider == "openai":
            settings.OPENAI_API_KEY = api_key
            self.openai_client = AsyncOpenAI(
                api_key=api_key,
                base_url=settings.OPENAI_API_BASE,
                timeout=300.0  # 5åˆ†é’Ÿè¶…æ—¶
            )
        
        # Save to file
        self._save_config_to_file()
        
        logger.info(f"LLM config updated: provider={self.provider}, model={self.model}")
    
    def get_current_config(self) -> dict:
        """
        Get current LLM configuration.
        """
        return {
            "provider": self.provider,
            "model": self.model,
            "display_name": "CBIT CBIT-Elite",  # Display name for UI
            "api_key_set": bool(settings.OPENAI_API_KEY) if self.provider == "openai" else None
        }

llm_service = LLMService()
